{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HACETTEPE UNIVERSITY\n",
    "\n",
    "## DEPARMENT OF COMPUTER ENGINEERING\n",
    "\n",
    "### BBM 409 : Machine Learning Lab 2nd Assignment Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - I : Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Question : MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " L(theta\\x1,...,xn) = P(x1).P(x2)P(x3)....P(xn) = (2theta/3)^f0 + (1theta/3)^f1  (2(1-theta/3)^f2 ((1-theta/3))^f3 \n",
    "                    = constant x theta^(f0+f1)(1-theta)^(f2+f3)\n",
    " \n",
    " \n",
    " (f0, f1, f2, f3 are respectively the frequencies of 0,1,2, and 3.)\n",
    " \n",
    " log L = constant + (f0 + f1)log(theta) - (f2 +f3)log(1-theta)\n",
    " \n",
    " dlogL / d(theta) = (f0 + f1)/theta + (f2 + f3)(1-theta) = 0\n",
    " \n",
    " (f0 + f1)log(1-theta) - (f2 +f3)log(theta) = 0\n",
    " \n",
    " (f0 + f1) = (f0 + f1 +f2 +f3)theta = n.theta\n",
    " \n",
    " theta = (f0 +f1)/n\n",
    " \n",
    " n is the number of observations and f0 and f1 are respectively the number of 0’s and number of 1’s\n",
    " \n",
    " n this sample, f0 = 2, f1 =3, f2 = 3, f3 = 2, n = 10\n",
    " \n",
    " (f0 +f1)/n = (2+3)/10 = 0.5\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - II : Detection of Fake News\n",
    "\n",
    "In this part , the code implements the Naive Bayes algorithm with using Unigram and Bigram features as BoW methods\n",
    "\n",
    "The code reads the trainSet and testSet . Creating Features it detects the News , fake or real ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "import math\n",
    "import collections\n",
    "import math\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def readFiles():\n",
    "\n",
    "    list_real = []\n",
    "    list_fake = []\n",
    "    list_test = []\n",
    "    with open('⁨⁨data⁩/clean_fake-Train.txt', 'r+') as f:\n",
    "        for line in f.readlines():\n",
    "            list_fake.append(line)\n",
    "    with open('data/clean_real-Train.txt', 'r+') as f:\n",
    "        for line in f.readlines():\n",
    "            list_real.append(line)\n",
    "\n",
    "    testFrame = pd.read_csv('data/test.csv')\n",
    "    testFrame.columns = ['Id', 'Category']\n",
    "    list_test = testFrame['Id'].tolist()\n",
    "    return list_real, list_fake,list_test, testFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code read the files from data folder and create the lists and dataFrame to hold the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCount(word, dict):\n",
    "    for key in dict.keys():\n",
    "        if word == key:\n",
    "            return dict[key][0]\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The function search on the dictionary and if the key is matched , it returns the count of key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findKeyWords(list):\n",
    "    words = []\n",
    "    for i in range(len(list)):\n",
    "\n",
    "        array = list[i].split()\n",
    "\n",
    "        for j in range(len(array)):\n",
    "            words.append(array[j])\n",
    "    frequencyOfWords = collections.Counter()\n",
    "    frequencyOfWords.update(words)\n",
    "\n",
    "    print(frequencyOfWords.most_common(5)[2:5])\n",
    "    return frequencyOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findkeyWords function count the duplicate elements and convert to dictionary with elements of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addUnseenWords(test,real,fake):\n",
    "\n",
    "    for key in test.keys():\n",
    "        if not key in real.keys():\n",
    "            real[key] = 1\n",
    "        if not key in fake.keys():\n",
    "            fake[key] = 1\n",
    "    return real, fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addUnseenWords apply the Laplace smoothing if any unseen items are in the trainSet(fake or real), the code adds to unigram model and frequency of these words is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildUnigram(frequency, ngramDict):\n",
    "    totalCount = sum(frequency.values());\n",
    "    for key, value in frequency.items():\n",
    "        # Laplace Smoothing #\n",
    "        ngramDict[key] = [(value + 1), math.log(value / (totalCount + len(frequency.keys())))]\n",
    "    return ngramDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buildUnigram function implements the unigram feature according to values and probablities of  the elements in trainSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBigram(list, grams, bigramDict):\n",
    "    bigram = []\n",
    "    for i in range(len(list)):\n",
    "        array = list[i].split()\n",
    "        bigram.append(ngrams(array, 2))\n",
    "    for i in range(len(bigram)):\n",
    "        for gram in bigram[i]:\n",
    "            grams.append(gram)\n",
    "    frequencyOfBiGram = collections.Counter()\n",
    "    frequencyOfBiGram.update(grams)\n",
    "    for key, value in frequencyOfBiGram.items():\n",
    "        bigramDict[key] = value\n",
    "    return bigramDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buildBigram function implements the bigram feature with using frequency of the words . The code uses the ngram function\n",
    "of nltk library to create the bigram models . collection.counter produces  unique elements with counts and these items\n",
    "are held in the bigram Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateProbablity(testNgram, realNgram, fakeNgram, probReal, probFake):\n",
    "    for grams in testNgram:\n",
    "        grams = ''.join(grams)\n",
    "        if grams in realNgram.keys():\n",
    "            probReal = probReal + realNgram[grams][1]\n",
    "\n",
    "        if grams in fakeNgram.keys():\n",
    "            probFake = probFake + fakeNgram[grams][1]\n",
    "\n",
    "    if probReal == max(probFake, probReal):\n",
    "        return \"real\"\n",
    "    else:\n",
    "        return \"fake\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate Probability function calculates the probablities of testSet elements according to unigram model and check the minimum value to compare testSet categories . The code calculate with using logarithmic function so all probabilities are added and first value of probReal and probFake equal to probablity of category in trainSet elements . It returns the label function to find the accuracy . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsLabelUni(testFrame, frequencyReal, frequencyFake, unigramDictReal, unigramDictFake, probReal, probFake):\n",
    "    right_prediction = 0\n",
    "    for i in range(len(testFrame.Id)):\n",
    "        testUnigram = ngrams(testFrame.Id[i].split(), 1)\n",
    "\n",
    "        if calculateProbablity(testUnigram, frequencyReal, frequencyFake, unigramDictReal, unigramDictFake, probReal,\n",
    "                               probFake) == \\\n",
    "                testFrame.Category[i]:\n",
    "            right_prediction = right_prediction + 1\n",
    "    print(\"Accuracy of the Unigram Model : \" + str(round(100 * (right_prediction / len(testFrame.Id)))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newsLabelUni function calculate the accuracy of the unigram model and check the categories and predictions of the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateProbBigram(testNgram, frequencyReal, frequencyFake, uniReal, uniFake, realNgram, fakeNgram, probReal,\n",
    "                        probFake):\n",
    "    deliminator = len(frequencyReal)\n",
    "    deliminatorFake = len(frequencyFake)\n",
    "\n",
    "    for grams in testNgram:\n",
    "\n",
    "        if grams in realNgram.keys():\n",
    "            probReal = probReal + math.log((realNgram[grams] + 1) / (findCount(grams[0], uniReal) + deliminator))\n",
    "        else:\n",
    "\n",
    "                probReal = probReal + math.log(1 / (findCount(grams[0], uniReal) + deliminator))\n",
    "\n",
    "        if grams in fakeNgram.keys():\n",
    "            probFake = probFake + math.log((fakeNgram[grams] + 1) / (findCount(grams[0], uniFake) + deliminatorFake))\n",
    "        else:\n",
    "                probFake = probFake + math.log(1 / (deliminatorFake + findCount(grams[0], uniFake)))\n",
    "\n",
    "    if probReal == max(probFake, probReal):\n",
    "        return \"real\"\n",
    "    else:\n",
    "        return \"fake\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The calculateProbBigram function labels the testSet element with using bigram feature , probabilities is calculated\n",
    "logarithmic function and picks the bigger one because all probabilities are between 0 and 1 , so they were negative \n",
    "expressions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsLabelBi(testFrame, frequencyReal, frequencyFake, unigramDictReal, unigramDictFake, bigramDictReal,\n",
    "                bigramDictFake, probReal, probFake):\n",
    "    right_prediction = 0\n",
    "    realWords = []\n",
    "    fakeWords = []\n",
    "\n",
    "    for i in range(len(testFrame.Id)):\n",
    "        testFrame.Id[i] = \"<s> \" + testFrame.Id[i] + \" <n>\"\n",
    "        words = testFrame.Id[i].split()\n",
    "        testBigram = ngrams(testFrame.Id[i].split(), 2)\n",
    "        label = calculateProbBigram(testBigram, frequencyReal, frequencyFake, unigramDictReal, unigramDictFake,\n",
    "                                    bigramDictReal, bigramDictFake, probReal, probFake)\n",
    "\n",
    "        if label == testFrame.Category[i]:\n",
    "            right_prediction = right_prediction + 1\n",
    "\n",
    "        if label == \"real\":\n",
    "            for i in range(len(words)):\n",
    "                realWords.append(words[i])\n",
    "        else:\n",
    "            for i in range(len(words)):\n",
    "                fakeWords.append(words[i])\n",
    "\n",
    "    print(\"Accuracy of the Bigram Model  : \" + str(round(100 * (right_prediction / len(testFrame.Id)))) + \"%\")\n",
    "    listWords(realWords, fakeWords, unigramDictReal, unigramDictFake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newsLabelBi function calculate the accuracy of the bigram model and check the categories and predictions of the code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listWords(realWords, fakeWords, dictReal, dictFake):\n",
    "    absentReal = {}\n",
    "    absentFake = {}\n",
    "\n",
    "    for key,value in dictReal.items():\n",
    "        if not key in realWords:\n",
    "            absentReal[key] = value[0]\n",
    "\n",
    "    for key,value in dictFake.items():\n",
    "        if not key in fakeWords:\n",
    "            absentFake[key] = value[0]\n",
    "\n",
    "    frequencyOfReals = collections.Counter()\n",
    "    frequencyOfReals.update(realWords)\n",
    "\n",
    "    frequencyOfFakes = collections.Counter()\n",
    "    frequencyOfFakes.update(fakeWords)\n",
    "    # The code sorts the dictionary according to its value #\n",
    "    sortedAbsentReal = [(k, absentReal[k]) for k in sorted(absentReal, key=absentReal.get, reverse=True)]\n",
    "    sortedAbsentFake = [(k, absentFake[k]) for k in sorted(absentFake, key=absentFake.get, reverse=True)]\n",
    "\n",
    "    print(\"List the 10 words whose presence most strongly predicts that the news is real :\")\n",
    "    print(frequencyOfReals.most_common(10))\n",
    "    print(\"List the 10 words whose presence most strongly predicts that the news is fake :\")\n",
    "    print(frequencyOfFakes.most_common(10))\n",
    "    print(\"List the 10 words whose absence most strongly predicts that the news is real :\")\n",
    "    print(sortedAbsentReal[:10])\n",
    "    print(\"List the 10 words whose absence most strongly predicts that the news is fake :\")\n",
    "    print(sortedAbsentFake[:10])\n",
    "\n",
    "    # Effects of Stop Words #\n",
    "    add = [\"<s>\", \"<n>\"]\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(add)\n",
    "    filtered_realNews = {}\n",
    "    filtered_fakeNews = {}\n",
    "\n",
    "    for key, value in frequencyOfReals.items():\n",
    "        if key not in stop_words:\n",
    "            filtered_realNews[key] = value\n",
    "\n",
    "    for key, value in frequencyOfFakes.items():\n",
    "        if key not in stop_words:\n",
    "            filtered_fakeNews[key] = value\n",
    "\n",
    "    filteredReals = collections.Counter()\n",
    "    filteredReals.update(filtered_realNews)\n",
    "\n",
    "    filteredFakes = collections.Counter()\n",
    "    filteredFakes.update(filtered_fakeNews)\n",
    "    print(\"List the 10 non-stopwords that most strongly predict that the news is real :\")\n",
    "    print(filteredReals.most_common(10))\n",
    "    print(\"List the 10 non-stopwords that most strongly predict that the news is fake :\")\n",
    "    print(filteredFakes.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listWords function creates two lists and holds the elements of testsets as real labeled or fake labeled . \n",
    "After that , the code find the words which are in trainSet but not in realNewsLabel . Same method is applied for realFakeLabel and to find most strongly 10 words for prediction , frequency of words are used . Counter function of collection library is used to count words ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Effects of StopWords\n",
    "\n",
    "Classification refers to compare the textfile to compare the each other which is used for prediction of test elements.\n",
    "Context of train sets include many meanless word and these may  confuse the calculation and we can use the resources effectively on the other hand simple words may have a big importance to understand the test elements . For example : \n",
    "\" good \" or \" not good\" are the similar expression but their means are opposite . That's situation is the make  a conflict for prediction .\n",
    "As we see the results , Stop words produce the better solution to find the words which have bigger probability for prediction . Because this project based on binary classification and it is not important meaning of texts . However we  assume that the code implements a sentiment classfication , probably stopwords produce the wrong results . It depends on our mission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of dictionaries to hold the N-Gram Models (Unigram and Bigram)\n",
    "unigramDictReal = {}\n",
    "unigramDictFake = {}\n",
    "unigramDictTest ={}\n",
    "bigramDictReal = {}\n",
    "bigramDictFake = {}\n",
    "gramsReal = []\n",
    "gramsFake = []\n",
    "\n",
    "list_real, list_fake,list_test, testFrame = rd.readFiles()\n",
    "\n",
    "\n",
    "# add a specific expression to detect head of the sentence  #\n",
    "for i in range(len(list_real)):\n",
    "    list_real[i] = list_real[i][0:-2]\n",
    "    list_real[i] = \"<s> \" + list_real[i]+\" <n>\"\n",
    "for j in range(len(list_fake)):\n",
    "    list_fake[j] = list_fake[j][0:-2]\n",
    "    list_fake[j] = \"<s> \"+ list_fake[j]+\" <n>\"\n",
    "\n",
    "# Most commonly keywords #\n",
    "print(\"Most commonly 3 keywords for Real News:\")\n",
    "frequencyReal = findKeyWords(list_real)\n",
    "print(frequencyReal.most_common(5)[2:5])\n",
    "print(\"Most commonly 3 keywords for Fake News:\")\n",
    "frequencyFake = findKeyWords(list_fake)\n",
    "print(frequencyFake.most_common(5)[2:5])\n",
    "\n",
    "frequencyTest = findKeyWords(list_test)\n",
    "frequencyReal, frequencyFake = ng.addUnseenWords(frequencyTest, frequencyReal, frequencyFake)\n",
    "unigramDictReal = buildUnigram(frequencyReal, unigramDictReal)\n",
    "unigramDictFake = buildUnigram(frequencyFake, unigramDictFake)\n",
    "\n",
    "bigramDictReal = buildBigram(list_real, gramsReal, bigramDictReal)\n",
    "bigramDictFake = buildBigram(list_fake, gramsFake, bigramDictFake)\n",
    "\n",
    "probReal = len(list_real) / (len(list_real)+len(list_fake))\n",
    "probFake = len(list_fake) / (len(list_real)+len(list_fake))\n",
    "\n",
    "newsLabelUni(testFrame, unigramDictReal, unigramDictFake\n",
    "                , math.log(probReal), math.log(probFake))\n",
    "\n",
    "newsLabelBi(testFrame, frequencyReal, frequencyFake,unigramDictReal,unigramDictFake, bigramDictReal, bigramDictFake\n",
    "                , math.log(probReal), math.log(probFake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code starts from this script . To detect the head of lines and the end of lines the code adds the special expressions . For head of line :  \"s\" and for end of line : \"n\" are added before create the unigram and bigram models . probsReal and probsFake variables hold the probability of their categories according to counts of the lines ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Unigram Model\n",
    "\n",
    "One of its characteristics is that it doesn't take the ordering of the words into account, so the order doesn't make a difference in how words are tagged or split up.\n",
    "\n",
    "### Bigram Model\n",
    "\n",
    "Bigram Models, on the other hand do care about the order of the words, so it considers the context of each word by analyzing it by pairs. Whereas a unigram model will tag a word independent of the other words .\n",
    "\n",
    " In many instances the order of the words might not matter at all. But sometimes meaning is very important to detect the category of the test elements so the bigram model is more successful approach than unigram model .\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
